{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Nsmc 데이터 로드\n",
    "train_df = pd.read_csv('./data/ratings_train.csv')\n",
    "train_df.dropna(inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 전처리 과정\n",
    "\n",
    "1. 한글만 남김 (특수 기호 제거)\n",
    "2. 명사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규식 이용하여 한글만 추출 : \"[^ㄱ-ㅎㅏ-ㅣ가-힣]\"\n",
    "\n",
    "train_df['document'] = train_df['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]+', \" \")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '을', '를', '는', '으로']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석 및 불용어 제거하여 데이터 전처리하기\n",
    "# 1. 문서 하나씩 읽어오기 (50개만)\n",
    "# 2. 문서에 okt 형태소 분석 적용 (morphs함수, stem=True 변경)\n",
    "# 3. 불용어 제거\n",
    "# 4. tokenized_words 리스트(문서 별 형태소 추출): [[words, words, words], [words, words, words]]\n",
    "# 5. sentence_nouns 리스트(문서 별 명사 리스트): [[noun, noun, noun, noun], [noun, noun, noun, noun]]\n",
    "okt = Okt()\n",
    "tokenized_words, sentence_nouns = [] , []\n",
    "\n",
    "\n",
    "for sentence in train_df['document'][:50]:\n",
    "    # 형태소 분석 결과\n",
    "    tokenized_words = okt.morphs(sentence, stem=True) # stem= True -> 어간추출\n",
    "    # 불용어 제거\n",
    "    stopwords_removed_sentence = [word for word in tokenized_words if not word in stopwords]\n",
    "    tokenized_words.append(stopwords_removed_sentence)\n",
    "\n",
    "    # 명사 추출 결과\n",
    "    nouns = okt.nouns(sentence)\n",
    "    sentence_nouns.append(nouns)\n",
    "# 딥러닝 max length : 문서의 최대길이로 맞춰서 픽스하는데\n",
    "# 이 길이보다 작으면 패딩(0)으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from konlpy.tag import Okt\n",
    "# import re\n",
    "\n",
    "# # Nsmc 데이터 로드\n",
    "# train_df = pd.read_csv('./data/ratings_train.csv')\n",
    "# train_df.dropna(inplace=True)\n",
    "\n",
    "# # 정규식을 이용하여 한글만 추출\n",
    "# def extract_korean(text):\n",
    "#     korean_pattern = re.compile('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]+')\n",
    "#     return korean_pattern.sub(\"\", text)\n",
    "\n",
    "# # 데이터 샘플링 및 한글 추출\n",
    "# sample_data = train_df['document'][:50].astype(str)\n",
    "# korean_sample_data = sample_data.apply(extract_korean)\n",
    "\n",
    "# # 불용어 정의\n",
    "# stopwords = ['의', '가', '이', '은', '들', '는', '좀', '을', '를', '는', '으로']\n",
    "\n",
    "# # 형태소 분석 및 불용어 제거하여 데이터 전처리하기\n",
    "# tokenized_words = []\n",
    "# sentence_nouns = []\n",
    "# okt = Okt()\n",
    "\n",
    "# # 각 문서에 대해 처리\n",
    "# for document in korean_sample_data:\n",
    "#     # 형태소 분석 및 불용어 제거\n",
    "#     words = [word for word in okt.morphs(document, stem=True) if word not in stopwords]\n",
    "    \n",
    "#     # 형태소 추출 결과를 tokenized_words 리스트에 추가\n",
    "#     tokenized_words.append(words)\n",
    "    \n",
    "#     # 명사 추출\n",
    "#     nouns = [word for word, pos in okt.pos(document) if pos == 'Noun']\n",
    "    \n",
    "#     # 명사 리스트를 sentence_nouns 리스트에 추가\n",
    "#     sentence_nouns.append(nouns)\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"tokenized_words:\", tokenized_words)\n",
    "# print(\"sentence_nouns:\", sentence_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_nouns(sample_data2):\n",
    "\n",
    "    print(f'hananum: {hananum.nouns(sample_data2)}')\n",
    "    print(f'kkma: {kkma.nouns(sample_data2)}')\n",
    "    print(f'komoran: {komoran.nouns(sample_data2)}')\n",
    "    print(f'okt: {okt.nouns(sample_data2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기초 자연어 처리\n",
    "\n",
    "1. 텍스트 데이터의 수를 기준(통계적 수치 확인)  \n",
    "    1. 텍스트 길이의 최대, 최소, 중앙값, 평균값을 확인  \n",
    "        - 최대값 : 임베딩을 할 때, max_len 파라미터에 대한 값을 설정\n",
    "    2. 각 품사별 데이터 수 확인\n",
    "        - 대부분은 명사가 많이 사용됨\n",
    "        - 만약 하드웨어등의 사양이 낮아 문제가 되는 경우, 명사만 분석을 하기 위해 데이터 수 확인  \n",
    "  \n",
    "2. 용어(텍스트) 기준  \n",
    "    1. 불용어 사용할 단어\n",
    "    2. 유의어들 확인하기 위한 원문 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 단어 길이: 60, 가장 짧은 단어 길이: 1\n",
      "가장 긴 명사 길이: 29, 가장 짧은 명사 길이: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n결과\\n가장 긴 단어 길이 : 95, 가장 짧은 단어 길이 : 1\\n가장 긴 명사 길이 : 66, 가장 짧은 명사 길이 : 0\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰의 최대 길이\n",
    "word_len = [len(l) for l in tokenized_words]\n",
    "nouns_len = [len(l) for l in sentence_nouns]\n",
    "\n",
    "print(f\"가장 긴 단어 길이: {max(word_len)}, 가장 짧은 단어 길이: {min(word_len)}\")\n",
    "print(f\"가장 긴 명사 길이: {max(nouns_len)}, 가장 짧은 명사 길이: {min(nouns_len)}\")\n",
    "\n",
    "\n",
    "'''\n",
    "결과\n",
    "가장 긴 단어 길이 : 95, 가장 짧은 단어 길이 : 1\n",
    "가장 긴 명사 길이 : 66, 가장 짧은 명사 길이 : 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 훈련\n",
    "\n",
    "- vector_size = 워드 벡터의 특징 값, 즉 임베딩 된 벡터의 차원  (128,512)\n",
    "- window = 컨텐스트 윈도우 크기 (학습할 때 주변 단위를 얼마큼 볼 것이냐)\n",
    "- min_count = 단어 최소 빈도 수 제한(빈도가 적은 단어들을 학습하지 않도록 하는 기준)  \n",
    "- workers = 학습을 위한 프로세스 수 (workers로 설정을 하거나 혹은 multiprocessing으로 병렬처리)  \n",
    "- sg = 0 -(CBOW) , 1 -(Skip-gram)  \n",
    "- (Skip-gram)  이 성능이 높음 (중심단어를 통해 주변 단어를 예측하는 게 성능이 좋음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 모델 훈련하기\n",
    "model_cbow = Word2Vec(tokenized_words, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "model_skipgram = Word2Vec(tokenized_words, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100)\n",
      "(4, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n결과\\n(15204, 100) 15204개의 단어를 100 차원으로 만듦\\n(15204, 100)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완성된 임베딩 매트릭스의 크기 확인(단어수, 차원수)\n",
    "print(model_cbow.wv.vectors.shape)\n",
    "print(model_skipgram.wv.vectors.shape)\n",
    "'''\n",
    "결과\n",
    "(15204, 100) 15204개의 단어를 100 차원으로 만듦\n",
    "(15204, 100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 저장 방법\n",
    "\n",
    "1. 재훈련 없이 모델만 사용\n",
    "2. 재훈련을 할 수 있도록 임베딩에 대한 기본 정보를 함께 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 방법 1 : 재훈련을 하지 않는 경우\n",
    "## Ram에 로드하지 않고도 디스크나 네트워크에서 데이터를 즉시 읽어 반복할 수 있음\n",
    "## 모델 inference 에서 활용하면 좋음\n",
    "\n",
    "model_cbow.wv.save_word2vec_format('./data/kor_w2v_cbow')\n",
    "model_skipgram.wv.save_word2vec_format('./data/kor_w2v_skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 방법1의 결과를 다시 로드해서 사용\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 모델 불러오기\n",
    "model_cbow_kv= KeyedVectors.load_word2vec_format('./data/kor_w2v_cbow')\n",
    "model_skipgram_kv= KeyedVectors.load_word2vec_format('./data/kor_w2v_skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('차승원', 0.8818742632865906), ('김명민', 0.8706954121589661), ('김혜수', 0.8630266189575195), ('최민수', 0.8564978241920471), ('설경구', 0.8538017272949219)]\n",
      "[('안성기', 0.8590131998062134), ('한석규', 0.8429045081138611), ('정재영', 0.838813841342926), ('황정민', 0.8353441953659058), ('류덕환', 0.8253893852233887)]\n"
     ]
    }
   ],
   "source": [
    "# 특정 단어를 중심으로 유사한 단어 확인하기\n",
    "print(model_cbow_kv.most_similar(\"송강호\", topn=5))\n",
    "print(model_skipgram_kv.most_similar(\"송강호\", topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 방법2 : 재훈련 가능한 모델\n",
    "model_cbow.save('./data/kor_w2v_cbow.model')\n",
    "model_skipgram_kv.save('./data/kor_w2v_skipgram.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "## 모델에 vocab 정보 등이 함께 저장된 결과를 로드함\n",
    "model_cbow_model = Word2Vec.load('./data/kor_w2v_cbow.model')\n",
    "model_skipgram_model = Word2Vec.load('./data/kor_w2v_skipgram.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('차승원', 0.8818742632865906), ('김명민', 0.8706954121589661), ('김혜수', 0.8630266189575195), ('최민수', 0.8564978241920471), ('설경구', 0.8538017272949219)]\n",
      "[('안성기', 0.8590131998062134), ('한석규', 0.8429045081138611), ('정재영', 0.838813841342926), ('황정민', 0.8353441953659058), ('류덕환', 0.8253893852233887)]\n"
     ]
    }
   ],
   "source": [
    "# 특정 단어를 중심으로 유사한 단어 확인하기\n",
    "print(model_cbow_model.wv.most_similar(\"송강호\", topn=5))\n",
    "print(model_skipgram_model.wv.most_similar(\"송강호\", topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47882542"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 단어 간의 유사도 파악하기 - 코사인 유사도\n",
    "model_skipgram_model.wv.similarity(\"추천\", \"강력\")\n",
    "\n",
    "model_skipgram_model.wv.similarity(\"망작\", \"쓰레기\")\n",
    "model_skipgram_model.wv.similarity(\"노잼\", \"쓰레기\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('최고다', 0.772616446018219),\n",
       " ('쵝오', 0.7664356827735901),\n",
       " ('단연', 0.7442367076873779),\n",
       " ('손꼽다', 0.718795657157898),\n",
       " ('꼽는', 0.7099205255508423),\n",
       " ('최강', 0.709646999835968),\n",
       " ('으뜸', 0.6837571859359741),\n",
       " ('최고봉', 0.6809198260307312),\n",
       " ('쵝', 0.6775249242782593),\n",
       " ('연애시대', 0.6722466349601746)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전에 없는 경우 확인\n",
    "model_skipgram_model.wv.most_similar(\"최고\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('노답', 0.7824556827545166),\n",
       " ('렉', 0.7778365612030029),\n",
       " ('좆', 0.777439296245575),\n",
       " ('개핵', 0.7581625580787659),\n",
       " ('핵', 0.7469645142555237),\n",
       " ('짱깨', 0.7467575669288635),\n",
       " ('극혐', 0.7430484294891357),\n",
       " ('씨발', 0.736915647983551),\n",
       " ('날림', 0.7270280718803406),\n",
       " ('절때', 0.7269129753112793)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 단어와 유사한 embedding 가져오기\n",
    "model_skipgram_model.wv.most_similar(positive=[\"여자\", \"감독\"]) # 여자, 감독과 유사한 임베딩\n",
    "model_skipgram_model.wv.most_similar(positive=[\"여자\", \"감독\"] , negative=[\"남자\"])\n",
    "\n",
    "\n",
    "\n",
    "model_skipgram_model.wv.most_similar(positive=[\"노잼\", \"쓰레기\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('울음', 0.7214616537094116),\n",
       " ('주룩주룩', 0.7146351933479309),\n",
       " ('코끝', 0.7118856906890869),\n",
       " ('폭풍눈물', 0.7094402313232422),\n",
       " ('주르륵', 0.7090370059013367),\n",
       " ('멎다', 0.7052245736122131),\n",
       " ('회한', 0.6988445520401001),\n",
       " ('펑펑', 0.69825679063797),\n",
       " ('방울', 0.6977316737174988),\n",
       " ('하염없이', 0.6957030892372131)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosmul 사용해 보기\n",
    "model_skipgram_model.wv.most_similar_cosmul(positive=['남자', '감독'], negative=['여자'])\n",
    "model_skipgram_model.wv.most_similar_cosmul(positive=['눈물', '슬픔'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6814057\n",
      "0.870179\n",
      "0.5506598\n"
     ]
    }
   ],
   "source": [
    "# 그룹간의 유사도 측정\n",
    "print(model_skipgram_model.wv.n_similarity(['남자', '배우'], ['여자', '감독']))\n",
    "print(model_skipgram_model.wv.n_similarity(['콧물', '눈물'], ['펑펑', '울음']))\n",
    "print(model_skipgram_model.wv.n_similarity(['노잼', '쓰레기'], ['존잼', '꿀잼']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'밥'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 유사하지 않은 단어를 추출\n",
    "model_skipgram_model.wv.doesnt_match(['영화', '밥', '국밥', '소머리국밥'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 학습된 모델 확인 및 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300',\n",
      " 'conceptnet-numberbatch-17-06-300',\n",
      " 'word2vec-ruscorpora-300',\n",
      " 'word2vec-google-news-300',\n",
      " 'glove-wiki-gigaword-50',\n",
      " 'glove-wiki-gigaword-100',\n",
      " 'glove-wiki-gigaword-200',\n",
      " 'glove-wiki-gigaword-300',\n",
      " 'glove-twitter-25',\n",
      " 'glove-twitter-50',\n",
      " 'glove-twitter-100',\n",
      " 'glove-twitter-200',\n",
      " '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from pprint import pprint as pp\n",
    "\n",
    "\n",
    "pp(list(gensim.downloader.info()['models'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# vector load\n",
    "glove_vector_25= api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tumblr', 0.9367011189460754),\n",
       " ('facebook', 0.9184155464172363),\n",
       " ('insta', 0.9175950884819031),\n",
       " ('twitter', 0.9104822278022766),\n",
       " ('youtube', 0.9031587839126587),\n",
       " ('skype', 0.8910087943077087),\n",
       " ('hashtag', 0.8861388564109802),\n",
       " ('spam', 0.8861327171325684),\n",
       " ('link', 0.8775855302810669),\n",
       " ('chat', 0.8751716017723083)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사도 확인하기\n",
    "glove_vector_25.most_similar(\"instagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_vector_100= api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('birthday', 0.9259342551231384),\n",
       " ('day', 0.8549740314483643),\n",
       " ('bday', 0.8145427107810974),\n",
       " ('merry', 0.7886534333229065),\n",
       " ('love', 0.7855904698371887),\n",
       " ('wish', 0.7771798372268677),\n",
       " ('hope', 0.7660956978797913),\n",
       " ('thank', 0.7639737129211426),\n",
       " ('year', 0.7505833506584167),\n",
       " ('thanks', 0.7475083470344543)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vector_100.most_similar(\"happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "305494ec69d5ad97a583cc76e8fd52e450123bc765c435a27726a289dbe2d5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
