{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kJIRBmDqBiKH"
      },
      "source": [
        "## Transformer 구조 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "GvyQ7eeZCV62",
        "outputId": "892adc2b-e3d2-495d-b96f-0ce3669558d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\chunjae\\appdata\\roaming\\python\\python311\\site-packages (0.1.99)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FWT-P5l0CVde"
      },
      "source": [
        "#### 1. 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "colab_type": "code",
        "id": "88wMq4r-CdoG",
        "outputId": "3b5835e5-20d4-4fe1-ed10-acb7c192d726"
      },
      "outputs": [],
      "source": [
        "# data를 저장할 directory 확인\n",
        "data_dir = \"./data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mw-viWEe_-gb"
      },
      "source": [
        "#### 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AALbOYMx_-Fj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kOnh3BmoCzgh"
      },
      "source": [
        "#### 3. 폴더의 목록을 확인\n",
        "data_dir 목록을 확인 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "colab_type": "code",
        "id": "AtSMyMiqC1Zz",
        "outputId": "29803586-aeb3-44d0-e5a7-20b7e1047625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".DS_Store\n",
            "kor_w2v_cbow\n",
            "kor_w2v_cbow.model\n",
            "kor_w2v_skipgram\n",
            "kor_w2v_skipgram.model\n",
            "kowiki.model\n",
            "kowiki.txt\n",
            "kowiki.vocab\n",
            "naver.model\n",
            "naver.vocab\n",
            "naver_review.txt\n",
            "ratings_train.csv\n",
            "ratings_train.txt\n"
          ]
        }
      ],
      "source": [
        "for f in os.listdir(data_dir):\n",
        "  print(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Ly5K7I8DMy0"
      },
      "source": [
        "#### 4. Vocab 및 입력\n",
        "Sentencepiece를 활용해 미리 만든 voca를 로드함  \n",
        ": wiki corpus로 만들어 놓음\n",
        "\n",
        "로딩된 vocab을 이용해 input을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁겨울', '은', '▁추', '워', '요', '.']\n",
            "[3234, 3744, 205, 4081, 3902, 3730]\n",
            "['▁감', '기', '▁조', '심', '하', '세', '요', '.']\n",
            "[199, 3746, 54, 3974, 3736, 3826, 3902, 3730]\n",
            "torch.Size([2, 8])\n",
            "tensor([[3234, 3744,  205, 4081, 3902, 3730,    0,    0],\n",
            "        [ 199, 3746,   54, 3974, 3736, 3826, 3902, 3730]])\n"
          ]
        }
      ],
      "source": [
        "# vocab 만들기\n",
        "# 입력값 구성 -> 입력값에 대해서 인코딩 [인덱스 변환]-> 입력값 출력\n",
        "vocab_file = f\"{data_dir}/kowiki.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)\n",
        "\n",
        "# 입력 테스트\n",
        "lines = [\n",
        "    \"겨울은 추워요.\",\n",
        "    \"감기 조심하세요.\"\n",
        "]\n",
        "\n",
        "# input\n",
        "inputs = []\n",
        "for line in lines:\n",
        "    pieces = vocab.encode_as_pieces(line) # 해당 라인을 토큰으로 바꿈 (EX:_나는)\n",
        "    ids = vocab.encode_as_ids(line) # input은 index로 바꿈\n",
        "    inputs.append(torch.tensor(ids))\n",
        "    print(pieces)\n",
        "    print(ids)\n",
        "\n",
        "\n",
        "# 입력 길이를 맞춰주기 위해 padding 수행: input의 최대 길이에 맞춰서 패딩 수행\n",
        "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\\\n",
        "\n",
        "# shape\n",
        "print(inputs.size()) # 2*8 행렬 생성 / 배치사이즈 * input 최대 맥스 length(패딩포함)\n",
        "\n",
        "# 값\n",
        "print(inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wOqt_-1p_Z2L"
      },
      "source": [
        "#### 5. Embedding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aAz1I-mG_dSQ"
      },
      "source": [
        "#### - Input Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "7bjcpE8Z-208",
        "outputId": "b237843e-2451-4ce6-edc6-40ba85aab05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8007\n",
            "torch.Size([2, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "# input 값에 대한 embedding 수행 = 1) 일반적인 인풋에 대한 임베딩, 2) 포지션에 관한 임베딩\n",
        "# 일반적인 인풋에 대한 임베딩 \n",
        "n_vocab = len(vocab)\n",
        "print(n_vocab)\n",
        "d_hidn = 128 # 보통의 hidden size: 512\n",
        "nn_emb = nn.Embedding(n_vocab, d_hidn)# 임베딩 레이어를 초기화할 떄, 외부 임베딩을 사용하지 않고 입력된 텍스트에 대해 학습을 함\n",
        "input_embs= nn_emb(inputs)\n",
        "print(input_embs.size())\n",
        "\n",
        "# 임베딩 벡터는 어떻게 구현이 될까? \n",
        "# 임베딩 벡터는 구현이 되면 어떻게 쓰냐 -> voca가 있으면 처리를 함\n",
        "# 처음의 차원 보카의 사이즈(8007) -> 히든 사이즈 128\n",
        "# torch.Size([2, 8, 128]) 배치사이즈, input의 최대 length, 히든사이즈\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EBriAmpB_cHQ"
      },
      "source": [
        "##### - Position Embedding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. 문장의 position 별 angle 값을 구함  \n",
        "2. 구해진 angle 중 짝수 index의 값에 대한 sin 값을 구합니다.  \n",
        "3. 구해진 angle 중 홀수 index의 값에 대한 cos 값을 구합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NlCmyZrkPuOX"
      },
      "outputs": [],
      "source": [
        "\"\"\" sinusoid position embedding \"\"\"\n",
        "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
        "    def cal_angle(position, i_hidn):\n",
        "        return position / np.power(10000, 2 * (i_hidn // 2)) / d_hidn\n",
        "\n",
        "    def get_posi_anlge_vec(position):\n",
        "        return[cal_angle(position, i_hidn) for i_hidn in range(d_hidn)] # 히든사이즈 별 angle 값을 구하기 위해 for문 사용\n",
        "        \n",
        "\n",
        "    # 각 position에 대해서 dim 마다 angle 값을 구함\n",
        "    sinusoid_table = np.array([get_posi_anlge_vec(i_seq) for i_seq in range(n_seq)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "\n",
        "    return sinusoid_table\n",
        "\n",
        "\n",
        "#  시퀀스 내 각 위치에 대한 고유한 임베딩 테이블이 반환됩니다.\n",
        "#  이러한 위치 임베딩은 Transformer와 같은 모델에서 입력 토큰의 위치 정보를 캡처하는 데 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "colab_type": "code",
        "id": "x_MrxrxqVyra",
        "outputId": "4559b7b0-5e95-4aef-9587-087eb036bb89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 128)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\chunjae\\AppData\\Local\\Temp\\ipykernel_22004\\472403830.py:4: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return position / np.power(10000, 2 * (i_hidn // 2)) / d_hidn\n",
            "C:\\Users\\chunjae\\AppData\\Local\\Temp\\ipykernel_22004\\472403830.py:4: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  return position / np.power(10000, 2 * (i_hidn // 2)) / d_hidn\n",
            "C:\\Users\\chunjae\\AppData\\Local\\Temp\\ipykernel_22004\\472403830.py:12: RuntimeWarning: invalid value encountered in sin\n",
            "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:,0::2])\n",
            "C:\\Users\\chunjae\\AppData\\Local\\Temp\\ipykernel_22004\\472403830.py:13: RuntimeWarning: invalid value encountered in cos\n",
            "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n"
          ]
        }
      ],
      "source": [
        "n_seq = 64\n",
        "pos_encoding = get_sinusoid_encoding_table(n_seq, d_hidn)\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "\n",
        "# embedding 그림 출력\n",
        "# plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
        "# plt.xlabel('Depth')\n",
        "# plt.xlim((0, d_hidn))\n",
        "# plt.ylabel('Position')\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "GmeDRFKqikMy",
        "outputId": "f2ccb64e-8c2f-4468-e70e-58becf9a9965"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "expand(CPUBoolType{[1, 2, 8, 1]}, size=[8]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (4)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m pos_mask \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m pos_mask \u001b[38;5;241m=\u001b[39m pos_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 2D 텐서로 변경\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m pos_mask \u001b[38;5;241m=\u001b[39m pos_mask\u001b[38;5;241m.\u001b[39mexpand_as(positions)  \u001b[38;5;66;03m# inputs와 positions의 모양을 맞춤\u001b[39;00m\n\u001b[0;32m     27\u001b[0m positions\u001b[38;5;241m.\u001b[39mmasked_fill_(pos_mask, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# padding 값을 마스킹\u001b[39;00m\n\u001b[0;32m     28\u001b[0m pos_embs \u001b[38;5;241m=\u001b[39m nn_pos(positions)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: expand(CPUBoolType{[1, 2, 8, 1]}, size=[8]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (4)"
          ]
        }
      ],
      "source": [
        "# position embedding 구성\n",
        "pos_encoding = torch.FloatTensor(pos_encoding)\n",
        "\n",
        "# embedding layer 생성\n",
        "nn_pos = nn.Embedding.from_pretrained(pos_encoding, freeze=True) # freeze 학습을 하지 않겠다\n",
        "nn_pos\n",
        "\n",
        "# 각각의 input 값을 구하고, input 값의 포지션에 대한 임베딩을 가져오면 됨 \n",
        "\n",
        "# input: [2, 8]\n",
        "positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(1)).contiguous() + 1 #0~7 에 해당하는 input 값\n",
        "# 익스팬드는 차원을 확장하는데 2 바이 8\n",
        "# 8에 해당하는 내용을 2개로 확장한다\n",
        "# torch.arange() 함수를 사용하여 입력 시퀀스의 길이(inputs.size(1))만큼의 정수 시퀀스를 생성\n",
        "# 이 시퀀스는 0부터 시작하여 입력 시퀀스의 길이 - 1까지의 값을 가집니다 0~7\n",
        "# 따라서, 생성된 정수 시퀀스를 입력 시퀀스의 길이와 동일하게 확장합니다. + 1 이를 통해 입력 시퀀스의 길이만큼의 값이 반복되는 시퀀스를 생성\n",
        "#.contiguous(): 확장된 시퀀스를 연속된 메모리 공간에 배치합니다. \n",
        "# 이는 메모리 공간을 연속적으로 사용하여 효율적인 연산을 수행하기 위해 필요\n",
        "\n",
        "\n",
        "\n",
        "# 임베딩 백터를 만들어놓음\n",
        "# 인풋값에 적용할 것\n",
        "# 포지션 인풋은 0이냐 1이냐 3이냐 인풋값의 위치 정보\n",
        "# 0번째에 있는 임베딩 벡터 값을 가져오기 위해\n",
        "# 포지션 인풋값 = 위치 정보\n",
        "\n",
        "# position masking 인풋값이 있느냐 없느냐를 찾음\n",
        "pos_mask = inputs.eq(0)\n",
        "pos_mask = pos_mask.unsqueeze(0).unsqueeze(-1)  # 2D 텐서로 변경\n",
        "pos_mask = pos_mask.expand_as(positions)  # inputs와 positions의 모양을 맞춤\n",
        "positions.masked_fill_(pos_mask, 0)  # padding 값을 마스킹\n",
        "pos_embs = nn_pos(positions)\n",
        "\n",
        "print(inputs)\n",
        "print(positions)\n",
        "print(pos_embs.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZX63wiFfruY7"
      },
      "outputs": [],
      "source": [
        "# 초기 input 값 구성\n",
        "input_sums = input_embs + pos_embs\n",
        "print(input_sums.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rOYl2XUyt2xc"
      },
      "source": [
        "#### 6. Scale Dot Product Attention"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Exfnhn9vA1fl"
      },
      "source": [
        "##### Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "colab_type": "code",
        "id": "aaUTzxeSunt9",
        "outputId": "674683f7-1161-485a-f437-551381d8cdf6"
      },
      "outputs": [],
      "source": [
        "# input 입력 값을 만드는 과정\n",
        "Q = input_sums\n",
        "K = input_sums\n",
        "V = input_sums"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "st1N5098A7e1"
      },
      "source": [
        "##### Q * K-transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "DQmtb8hcxK1k",
        "outputId": "8f446a21-98c6-46c0-9713-a7af67446c22"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2e7jrCXpBCyS"
      },
      "source": [
        "##### Scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "colab_type": "code",
        "id": "tS_LXRbY5hcK",
        "outputId": "67123751-e82b-4a84-d861-5df6a037b7e3"
      },
      "outputs": [],
      "source": [
        "# scaled dot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lnQ7phqTBGJG"
      },
      "source": [
        "##### Mask (Opt.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "BENnRJdKxits",
        "outputId": "ee834637-15cf-4ec7-8f66-26716e903c43"
      },
      "outputs": [],
      "source": [
        "# masking"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GaAzVRmhBJ3B"
      },
      "source": [
        "##### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "HKl9riPRxqkj",
        "outputId": "85f1d9ab-ccfe-4f2a-c07e-ad32fb84713e"
      },
      "outputs": [],
      "source": [
        "# softmax 적용"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h-MwaJ1oBNOe"
      },
      "source": [
        "##### atten_prov * V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "P2aRJH-Rxyq8",
        "outputId": "10567189-7867-41ed-fde5-a45e24a8152c"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WR0Wh1ORBRJK"
      },
      "source": [
        "##### Implementation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EkXGHazDt1rb"
      },
      "outputs": [],
      "source": [
        "\"\"\" scale dot product attention \"\"\"\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_head):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "\n",
        "        return context, attn_prob"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yK38SMqGsiXL"
      },
      "source": [
        "#### 7. Multi-Head Attention"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i7F4mEgmBjuw"
      },
      "source": [
        "##### Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rZyMvxiathB-"
      },
      "outputs": [],
      "source": [
        "Q = input_sums\n",
        "K = input_sums\n",
        "V = input_sums\n",
        "# masking 만들기 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mly3n1YcBnR7"
      },
      "source": [
        "##### Multi Head Q, K, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "Pes5Jait2EXR",
        "outputId": "c527de1b-b28a-4885-ee72-3d980632223b"
      },
      "outputs": [],
      "source": [
        "# 멀티 헤드 수에 맞게 linear 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "oW9s0Oam3ZeG",
        "outputId": "ff64f15d-0c2f-487e-e45b-fbcb793b87f8"
      },
      "outputs": [],
      "source": [
        "# 멀티 헤드 수에 맞게 변경 -> Q, K, V 모두"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u73qrnhfBsYR"
      },
      "source": [
        "##### Multi Head Attention Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "n9ItmgzN32fr",
        "outputId": "1f32a3d3-a145-4671-c15d-a10635608777"
      },
      "outputs": [],
      "source": [
        "# Mask도 변경"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubh168SyBxnw"
      },
      "source": [
        "##### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "ufWp4KXo6yyi",
        "outputId": "52021897-5830-42e7-d407-fd5b2dff8876"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0zRmgcEfB1lM"
      },
      "source": [
        "##### Concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "EjQQPFsn7U8q",
        "outputId": "a3ac5624-652d-46bc-b5e3-b98f37b949e8"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6bxvZK6uB4WS"
      },
      "source": [
        "##### Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "kz3C9ABv7st1",
        "outputId": "d0ad361d-e594-409e-e3d6-29ecf84a110b"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wwcq-wgvB8Hq"
      },
      "source": [
        "##### Implementation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pVQZieaCB7xp"
      },
      "outputs": [],
      "source": [
        "\"\"\" multi head attention \"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_hidn, n_head, d_head):\n",
        "        super().__init__()\n",
        "        # self 인자\n",
        "        # linear, sacled_dot_attn, linear\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        batch_size = Q.size(0)\n",
        "        # q_s: (bs, n_head, n_q_seq, d_head)\n",
        "\n",
        "        # k_s: (bs, n_head, n_k_seq, d_head)\n",
        "\n",
        "        # v_s: (bs, n_head, n_v_seq, d_head)\n",
        "\n",
        "        # mask\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "\n",
        "        # scaled dot\n",
        "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
        "\n",
        "        # concat\n",
        "        # (bs, n_q_seq, h_head * d_head)\n",
        "\n",
        "        # linear\n",
        "        # (bs, n_q_seq, d_hidn)\n",
        "\n",
        "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        return output, attn_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4O7PH-v-SA9B"
      },
      "source": [
        "#### 8. Masked Multi Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "colab_type": "code",
        "id": "sg1uvsXJSAJE",
        "outputId": "d4bcd74c-7bb5-439b-bf71-2594cd4d850a"
      },
      "outputs": [],
      "source": [
        "\"\"\" attention decoder mask \"\"\"\n",
        "def get_attn_decoder_mask(seq):\n",
        "    return subsequent_mask\n",
        "\n",
        "\n",
        "Q = input_sums\n",
        "K = input_sums\n",
        "V = input_sums\n",
        "\n",
        "# attn_pad_mask : 기존 input에 대한 pad mask\n",
        "\n",
        "# attn_dec_mask : 현재 단어에서 이전 단어만 보겠다는 attention mask\n",
        "\n",
        "# attn_mask : attn_pad_mask + attn_dec_mask\n",
        "\n",
        "batch_size = Q.size(0)\n",
        "n_head = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "-YdCqAETUSzv",
        "outputId": "c927ad28-8967-4fc8-c614-6324467fa053"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xPiW76dzGy8D"
      },
      "source": [
        "#### 9. Feed Forward"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nLWqtn7UHGz-"
      },
      "source": [
        "##### f1 (Linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "U0iAPIfCHGaa",
        "outputId": "484ee583-3dc9-4401-9007-6f4c79c8cb7f"
      },
      "outputs": [],
      "source": [
        "# Linear : Conv1d로 활용\n",
        "# (bs, d_hidn * 4, n_seq)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezEDjlVtIqf0"
      },
      "source": [
        "##### Activation (relu or gelu)\n",
        "\n",
        "![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/activation.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "x7mTDJZiIOrL"
      },
      "outputs": [],
      "source": [
        "# active = F.gelu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fAcrKhAtIvn5"
      },
      "source": [
        "##### f3 (Linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "xGRsfIOcOgLR",
        "outputId": "427c31fb-2691-4524-f382-c0294143286c"
      },
      "outputs": [],
      "source": [
        "# Linear : Conv1d로 활용"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sE6-A3OhIzLo"
      },
      "source": [
        "##### Implementation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TaRkgC4CRs60"
      },
      "outputs": [],
      "source": [
        "\"\"\" feed forward \"\"\"\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, d_hidn):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # f1 output: (bs, d_ff, n_seq)\n",
        "\n",
        "        # f2 output: (bs, n_seq, d_hidn)\n",
        "\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        return output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transformer-01.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_lecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "305494ec69d5ad97a583cc76e8fd52e450123bc765c435a27726a289dbe2d5e0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
