{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kJIRBmDqBiKH"
      },
      "source": [
        "## Transformer 구조 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "GvyQ7eeZCV62",
        "outputId": "892adc2b-e3d2-495d-b96f-0ce3669558d5"
      },
      "outputs": [],
      "source": [
        "# !pip install sentencepiece"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FWT-P5l0CVde"
      },
      "source": [
        "#### 1. 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "colab_type": "code",
        "id": "88wMq4r-CdoG",
        "outputId": "3b5835e5-20d4-4fe1-ed10-acb7c192d726"
      },
      "outputs": [],
      "source": [
        "# data를 저장할 directory 확인\n",
        "data_dir = \"./data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mw-viWEe_-gb"
      },
      "source": [
        "#### 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AALbOYMx_-Fj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kOnh3BmoCzgh"
      },
      "source": [
        "#### 3. 폴더의 목록을 확인\n",
        "data_dir 목록을 확인 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "colab_type": "code",
        "id": "AtSMyMiqC1Zz",
        "outputId": "29803586-aeb3-44d0-e5a7-20b7e1047625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".DS_Store\n",
            "kor_w2v_cbow\n",
            "kor_w2v_cbow.model\n",
            "kor_w2v_skipgram\n",
            "kor_w2v_skipgram.model\n",
            "kowiki.model\n",
            "kowiki.txt\n",
            "kowiki.vocab\n",
            "naver.model\n",
            "naver.vocab\n",
            "naver_review.txt\n",
            "ratings_train.csv\n",
            "ratings_train.txt\n"
          ]
        }
      ],
      "source": [
        "for f in os.listdir(data_dir):\n",
        "  print(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Ly5K7I8DMy0"
      },
      "source": [
        "#### 4. Vocab 및 입력\n",
        "Sentencepiece를 활용해 미리 만든 voca를 로드함  \n",
        ": wiki corpus로 만들어 놓음\n",
        "\n",
        "로딩된 vocab을 이용해 input을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁겨울', '은', '▁추', '워', '요', '.']\n",
            "[3234, 3744, 205, 4081, 3902, 3730]\n",
            "['▁감', '기', '▁조', '심', '하', '세', '요', '.']\n",
            "[199, 3746, 54, 3974, 3736, 3826, 3902, 3730]\n",
            "torch.Size([2, 8])\n",
            "tensor([[3234, 3744,  205, 4081, 3902, 3730,    0,    0],\n",
            "        [ 199, 3746,   54, 3974, 3736, 3826, 3902, 3730]])\n"
          ]
        }
      ],
      "source": [
        "# vocab 만들기\n",
        "# 모델 로딩\n",
        "import csv\n",
        "vocab_file = f\"{data_dir}/kowiki.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)\n",
        "\n",
        "# 입력 텍스트\n",
        "lines = [\n",
        "    \"겨울은 추워요.\",\n",
        "    \"감기 조심하세요.\"\n",
        "]\n",
        "\n",
        "# input\n",
        "inputs = []\n",
        "for line in lines:\n",
        "    pieces = vocab.EncodeAsPieces(line) # 토큰으로 바꿈\n",
        "    ids = vocab.EncodeAsIds(line) # index로 바꿈\n",
        "    inputs.append(torch.tensor(ids))\n",
        "    print(pieces)\n",
        "    print(ids)\n",
        "    \n",
        "# 입력 길이를 맞춰주기 위해 Padding 수행 : 최대 길이에 맞춰서 패딩이 이루어짐\n",
        "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "\n",
        "# shape\n",
        "print(inputs.size()) \n",
        "\n",
        "# 값\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wOqt_-1p_Z2L"
      },
      "source": [
        "#### 5. Embedding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aAz1I-mG_dSQ"
      },
      "source": [
        "#### - Input Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "7bjcpE8Z-208",
        "outputId": "b237843e-2451-4ce6-edc6-40ba85aab05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8007\n",
            "torch.Size([2, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "# input 값에 대한 embedding\n",
        "n_vocab = len(vocab)\n",
        "print(n_vocab)\n",
        "d_hidn = 128 # hiddensize : 512\n",
        "nn_emb = nn.Embedding(n_vocab, d_hidn)\n",
        "input_embs = nn_emb(inputs)\n",
        "print(input_embs.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EBriAmpB_cHQ"
      },
      "source": [
        "##### - Position Embedding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. 문장의 position 별 angle 값을 구함  \n",
        "2. 구해진 angle 중 짝수 index의 값에 대한 sin 값을 구합니다.  \n",
        "3. 구해진 angle 중 홀수 index의 값에 대한 cos 값을 구합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NlCmyZrkPuOX"
      },
      "outputs": [],
      "source": [
        "\"\"\" sinusoid position embedding \"\"\"\n",
        "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
        "    def cal_angle(position, i_hidn):\n",
        "        return position/ np.power(10000, 2*(i_hidn //2) / d_hidn)\n",
        "    \n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
        "    # 각 possition에 대해서 dim 마다 angle 값을 구함\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])  \n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) \n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) \n",
        "    return sinusoid_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "colab_type": "code",
        "id": "x_MrxrxqVyra",
        "outputId": "4559b7b0-5e95-4aef-9587-087eb036bb89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 128)\n"
          ]
        }
      ],
      "source": [
        "n_seq = 64\n",
        "pos_encoding = get_sinusoid_encoding_table(n_seq, d_hidn)\n",
        "print(pos_encoding.shape)\n",
        "# embedding 그림 출력\n",
        "# plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
        "# plt.xlabel('Depth')\n",
        "# plt.xlim((0, d_hidn))\n",
        "# plt.ylabel('Position')\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "GmeDRFKqikMy",
        "outputId": "f2ccb64e-8c2f-4468-e70e-58becf9a9965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[3234, 3744,  205, 4081, 3902, 3730,    0,    0],\n",
            "        [ 199, 3746,   54, 3974, 3736, 3826, 3902, 3730]])\n",
            "tensor([[1, 2, 3, 4, 5, 6, 0, 0],\n",
            "        [1, 2, 3, 4, 5, 6, 7, 8]])\n",
            "torch.Size([2, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "# position embedding 구성\n",
        "pos_encoding = torch.FloatTensor(pos_encoding)\n",
        "\n",
        "# embedding layer 생성\n",
        "nn_pos  = nn.Embedding.from_pretrained(pos_encoding, freeze=True)\n",
        "\n",
        "# inputs: [2:8]\n",
        "positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
        "\n",
        "# position masking\n",
        "pos_mask = inputs.eq(0)\n",
        "positions.masked_fill_(pos_mask, 0) # padding 값을 MASKING\n",
        "pos_embs = nn_pos(positions)\n",
        "\n",
        "print(inputs)\n",
        "print(positions)\n",
        "print(pos_embs.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZX63wiFfruY7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 128])\n",
            "tensor([[[ 0.2217,  1.0359, -1.6467,  ...,  0.0962, -1.2751,  1.2077],\n",
            "         [ 0.0784,  0.5207,  0.5911,  ...,  0.5457, -0.5813,  1.4269],\n",
            "         [-0.8689, -0.3755, -0.7425,  ...,  1.8775, -0.0199,  0.8167],\n",
            "         ...,\n",
            "         [-0.6697,  0.2188, -1.3910,  ...,  0.4820, -2.9566,  1.5345],\n",
            "         [ 1.8596,  1.3996, -0.5283,  ...,  0.5701, -2.2868,  0.9269],\n",
            "         [ 1.8596,  1.3996, -0.5283,  ...,  0.5701, -2.2868,  0.9269]],\n",
            "\n",
            "        [[-0.2576,  0.0702, -1.2664,  ..., -0.5051, -1.0110,  1.9549],\n",
            "         [ 0.7859, -0.6066,  1.4636,  ..., -0.3887,  1.6415,  0.4860],\n",
            "         [ 2.1200, -4.0953,  1.3539,  ...,  2.8743,  1.1692,  0.3495],\n",
            "         ...,\n",
            "         [-0.5206,  0.0577, -0.5392,  ...,  1.9668, -0.9671,  0.8915],\n",
            "         [ 1.1237,  0.9027, -1.4957,  ...,  0.8373, -0.4433,  3.0406],\n",
            "         [ 0.5991, -0.8869,  0.0952,  ...,  0.4820, -2.9564,  1.5345]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[[-0.6198,  0.4956, -2.4084,  ..., -0.9038, -1.2752,  0.2077],\n",
            "         [-0.8309,  0.9368, -0.3959,  ..., -0.4543, -0.5815,  0.4269],\n",
            "         [-1.0100,  0.6145, -1.2598,  ...,  0.8775, -0.0202, -0.1833],\n",
            "         ...,\n",
            "         [-0.3903, -0.7414, -0.5056,  ..., -0.5180, -2.9573,  0.5345],\n",
            "         [ 1.8596,  0.3996, -0.5283,  ..., -0.4299, -2.2868, -0.0731],\n",
            "         [ 1.8596,  0.3996, -0.5283,  ..., -0.4299, -2.2868, -0.0731]],\n",
            "\n",
            "        [[-1.0991, -0.4701, -2.0282,  ..., -1.5051, -1.0111,  0.9549],\n",
            "         [-0.1234, -0.1904,  0.4766,  ..., -1.3887,  1.6413, -0.5140],\n",
            "         [ 1.9789, -3.1053,  0.8366,  ...,  1.8743,  1.1689, -0.6505],\n",
            "         ...,\n",
            "         [-0.2412, -0.9025,  0.3462,  ...,  0.9668, -0.9678, -0.1085],\n",
            "         [ 0.4667,  0.1488, -1.2761,  ..., -0.1627, -0.4441,  2.0406],\n",
            "         [-0.3903, -0.7414, -0.5056,  ..., -0.5180, -2.9573,  0.5345]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "tensor([[[ 8.4147e-01,  5.4030e-01,  7.6172e-01,  ...,  1.0000e+00,\n",
            "           1.1548e-04,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  9.8705e-01,  ...,  1.0000e+00,\n",
            "           2.3096e-04,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  5.1731e-01,  ...,  1.0000e+00,\n",
            "           3.4643e-04,  1.0000e+00],\n",
            "         ...,\n",
            "         [-2.7942e-01,  9.6017e-01, -8.8542e-01,  ...,  1.0000e+00,\n",
            "           6.9287e-04,  1.0000e+00],\n",
            "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "           0.0000e+00,  1.0000e+00],\n",
            "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "           0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[ 8.4147e-01,  5.4030e-01,  7.6172e-01,  ...,  1.0000e+00,\n",
            "           1.1548e-04,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  9.8705e-01,  ...,  1.0000e+00,\n",
            "           2.3096e-04,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  5.1731e-01,  ...,  1.0000e+00,\n",
            "           3.4643e-04,  1.0000e+00],\n",
            "         ...,\n",
            "         [-2.7942e-01,  9.6017e-01, -8.8542e-01,  ...,  1.0000e+00,\n",
            "           6.9287e-04,  1.0000e+00],\n",
            "         [ 6.5699e-01,  7.5390e-01, -2.1963e-01,  ...,  1.0000e+00,\n",
            "           8.0835e-04,  1.0000e+00],\n",
            "         [ 9.8936e-01, -1.4550e-01,  6.0082e-01,  ...,  1.0000e+00,\n",
            "           9.2383e-04,  1.0000e+00]]])\n"
          ]
        }
      ],
      "source": [
        "# 초기 input 값 구성\n",
        "input_sums = input_embs +pos_embs\n",
        "print(input_sums.size())\n",
        "print(input_sums)\n",
        "print(input_embs)\n",
        "print(pos_embs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rOYl2XUyt2xc"
      },
      "source": [
        "#### 6. Scale Dot Product Attention"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Exfnhn9vA1fl"
      },
      "source": [
        "##### Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "colab_type": "code",
        "id": "aaUTzxeSunt9",
        "outputId": "674683f7-1161-485a-f437-551381d8cdf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 8])\n",
            "tensor([[False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True]])\n"
          ]
        }
      ],
      "source": [
        "# input 입력 값을 만드는 과정\n",
        "Q = input_sums\n",
        "K = input_sums\n",
        "V = input_sums\n",
        "\n",
        "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0) , Q.size(1), K.size(1))\n",
        "print(attn_mask.size())\n",
        "print(attn_mask[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "st1N5098A7e1"
      },
      "source": [
        "##### Q * K-transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "DQmtb8hcxK1k",
        "outputId": "8f446a21-98c6-46c0-9713-a7af67446c22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 8])\n",
            "tensor([[181.1045,  54.7520,  51.1379,  55.8373,  69.7627,  63.9615,  78.3058,\n",
            "          78.3058],\n",
            "        [ 54.7520, 195.3990,  35.4192,  40.9941,  42.6141,  21.3285,  50.6848,\n",
            "          50.6848],\n",
            "        [ 51.1379,  35.4192, 178.9026,  61.1841,  78.2546,  47.3389,  61.2755,\n",
            "          61.2755],\n",
            "        [ 55.8373,  40.9941,  61.1841, 156.4162,  67.0148,  44.1819,  74.0393,\n",
            "          74.0393],\n",
            "        [ 69.7627,  42.6141,  78.2546,  67.0148, 233.2007,  94.2432,  75.7979,\n",
            "          75.7979],\n",
            "        [ 63.9615,  21.3285,  47.3389,  44.1819,  94.2432, 165.0052,  65.8726,\n",
            "          65.8726],\n",
            "        [ 78.3058,  50.6848,  61.2755,  74.0393,  75.7979,  65.8726, 189.4782,\n",
            "         189.4782],\n",
            "        [ 78.3058,  50.6848,  61.2755,  74.0393,  75.7979,  65.8726, 189.4782,\n",
            "         189.4782]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "scores = torch.matmul(Q, K.transpose(-1, -2))\n",
        "print(scores.size())\n",
        "print(scores[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2e7jrCXpBCyS"
      },
      "source": [
        "##### Scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "colab_type": "code",
        "id": "tS_LXRbY5hcK",
        "outputId": "67123751-e82b-4a84-d861-5df6a037b7e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 8])\n",
            "tensor([[22.6381,  6.8440,  6.3922,  6.9797,  8.7203,  7.9952,  9.7882,  9.7882],\n",
            "        [ 6.8440, 24.4249,  4.4274,  5.1243,  5.3268,  2.6661,  6.3356,  6.3356],\n",
            "        [ 6.3922,  4.4274, 22.3628,  7.6480,  9.7818,  5.9174,  7.6594,  7.6594],\n",
            "        [ 6.9797,  5.1243,  7.6480, 19.5520,  8.3769,  5.5227,  9.2549,  9.2549],\n",
            "        [ 8.7203,  5.3268,  9.7818,  8.3769, 29.1501, 11.7804,  9.4747,  9.4747],\n",
            "        [ 7.9952,  2.6661,  5.9174,  5.5227, 11.7804, 20.6257,  8.2341,  8.2341],\n",
            "        [ 9.7882,  6.3356,  7.6594,  9.2549,  9.4747,  8.2341, 23.6848, 23.6848],\n",
            "        [ 9.7882,  6.3356,  7.6594,  9.2549,  9.4747,  8.2341, 23.6848, 23.6848]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# scaled dot\n",
        "d_head = 64\n",
        "scores = scores.mul_(1/d_head**0.5)\n",
        "print(scores.size())\n",
        "print(scores[0])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lnQ7phqTBGJG"
      },
      "source": [
        "##### Mask (Opt.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "BENnRJdKxits",
        "outputId": "ee834637-15cf-4ec7-8f66-26716e903c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 8])\n",
            "tensor([[ 2.2638e+01,  6.8440e+00,  6.3922e+00,  6.9797e+00,  8.7203e+00,\n",
            "          7.9952e+00, -1.0000e+09, -1.0000e+09],\n",
            "        [ 6.8440e+00,  2.4425e+01,  4.4274e+00,  5.1243e+00,  5.3268e+00,\n",
            "          2.6661e+00, -1.0000e+09, -1.0000e+09],\n",
            "        [ 6.3922e+00,  4.4274e+00,  2.2363e+01,  7.6480e+00,  9.7818e+00,\n",
            "          5.9174e+00, -1.0000e+09, -1.0000e+09],\n",
            "        [ 6.9797e+00,  5.1243e+00,  7.6480e+00,  1.9552e+01,  8.3769e+00,\n",
            "          5.5227e+00, -1.0000e+09, -1.0000e+09],\n",
            "        [ 8.7203e+00,  5.3268e+00,  9.7818e+00,  8.3769e+00,  2.9150e+01,\n",
            "          1.1780e+01, -1.0000e+09, -1.0000e+09],\n",
            "        [ 7.9952e+00,  2.6661e+00,  5.9174e+00,  5.5227e+00,  1.1780e+01,\n",
            "          2.0626e+01, -1.0000e+09, -1.0000e+09],\n",
            "        [ 9.7882e+00,  6.3356e+00,  7.6594e+00,  9.2549e+00,  9.4747e+00,\n",
            "          8.2341e+00, -1.0000e+09, -1.0000e+09],\n",
            "        [ 9.7882e+00,  6.3356e+00,  7.6594e+00,  9.2549e+00,  9.4747e+00,\n",
            "          8.2341e+00, -1.0000e+09, -1.0000e+09]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# masking\n",
        "scores.masked_fill_(attn_mask, -1e9)\n",
        "print(scores.size())\n",
        "print(scores[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GaAzVRmhBJ3B"
      },
      "source": [
        "##### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "HKl9riPRxqkj",
        "outputId": "85f1d9ab-ccfe-4f2a-c07e-ad32fb84713e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 8])\n",
            "tensor([[1.0000e+00, 1.3827e-07, 8.8010e-08, 1.5836e-07, 9.0283e-07, 4.3720e-07,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [2.3159e-08, 1.0000e+00, 2.0664e-09, 4.1481e-09, 5.0792e-09, 3.5503e-10,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [1.1589e-07, 1.6246e-08, 1.0000e+00, 4.0685e-07, 3.4367e-06, 7.2081e-08,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [3.4664e-06, 5.4212e-07, 6.7631e-06, 9.9997e-01, 1.4018e-05, 8.0751e-07,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [1.3411e-09, 4.5047e-11, 3.8768e-09, 9.5126e-10, 1.0000e+00, 2.8605e-08,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [3.2703e-06, 1.5856e-08, 4.0946e-07, 2.7594e-07, 1.4404e-04, 9.9985e-01,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [3.7319e-01, 1.1816e-02, 4.4404e-02, 2.1894e-01, 2.7277e-01, 7.8882e-02,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [3.7319e-01, 1.1816e-02, 4.4404e-02, 2.1894e-01, 2.7277e-01, 7.8882e-02,\n",
            "         0.0000e+00, 0.0000e+00]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# softmax 적용\n",
        "attn_prob = nn.Softmax(dim=-1)(scores)\n",
        "print(attn_prob.size())\n",
        "print(attn_prob[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h-MwaJ1oBNOe"
      },
      "source": [
        "##### atten_prov * V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "P2aRJH-Rxyq8",
        "outputId": "10567189-7867-41ed-fde5-a45e24a8152c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "context = torch.matmul(attn_prob, V)\n",
        "print(context.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WR0Wh1ORBRJK"
      },
      "source": [
        "##### Implementation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EkXGHazDt1rb"
      },
      "outputs": [],
      "source": [
        "\"\"\" scale dot product attention \"\"\"\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_head):\n",
        "        super().__init__()\n",
        "        self.scale = 1/(d_head**0.5)   \n",
        "        \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn_prob, V)\n",
        "        return context, attn_prob\n",
        "    \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yK38SMqGsiXL"
      },
      "source": [
        "#### 7. Multi-Head Attention"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i7F4mEgmBjuw"
      },
      "source": [
        "##### Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rZyMvxiathB-"
      },
      "outputs": [],
      "source": [
        "Q = input_sums # [batch, n_seq, dim]\n",
        "K = input_sums\n",
        "V = input_sums\n",
        "# masking 만들기 \n",
        "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
        "\n",
        "batch_size = Q.size(0)\n",
        "n_head = 2 # 기존 논문 6\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mly3n1YcBnR7"
      },
      "source": [
        "##### Multi Head Q, K, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "Pes5Jait2EXR",
        "outputId": "c527de1b-b28a-4885-ee72-3d980632223b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 128])\n",
            "torch.Size([2, 8, 2, 64])\n",
            "torch.Size([2, 2, 8, 64])\n"
          ]
        }
      ],
      "source": [
        "# 멀티 헤드 수에 맞게 linear 구성\n",
        "W_Q = nn.Linear(d_hidn, n_head * d_head) \n",
        "W_K = nn.Linear(d_hidn, n_head * d_head)\n",
        "W_V = nn.Linear(d_hidn, n_head * d_head) \n",
        "\n",
        "# (bs, n_seq, n_head * d_head)\n",
        "q_s = W_Q(Q)\n",
        "print(q_s.size())\n",
        "\n",
        "# (bs, n_seq, n_head, d_head)\n",
        "q_s = q_s.view(batch_size, -1, n_head, d_head) # 배열을 4차원으로 변경\n",
        "print(q_s.size())\n",
        "\n",
        "# (bs, n_head, n_seq, d_head)\n",
        "q_s = q_s.transpose(1, 2)\n",
        "print(q_s.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "oW9s0Oam3ZeG",
        "outputId": "ff64f15d-0c2f-487e-e45b-fbcb793b87f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 8, 64]) torch.Size([2, 2, 8, 64]) torch.Size([2, 2, 8, 64])\n"
          ]
        }
      ],
      "source": [
        "# 멀티 헤드 수에 맞게 변경 -> Q, K, V 모두\n",
        "q_s = W_Q(Q).view(batch_size, -1, n_head, d_head).transpose(1, 2)\n",
        "k_s = W_K(K).view(batch_size, -1, n_head, d_head).transpose(1, 2)\n",
        "v_s = W_V(V).view(batch_size, -1, n_head, d_head).transpose(1, 2)\n",
        "print(q_s.size(), k_s.size(), v_s.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u73qrnhfBsYR"
      },
      "source": [
        "##### Multi Head Attention Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "n9ItmgzN32fr",
        "outputId": "1f32a3d3-a145-4671-c15d-a10635608777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 8])\n",
            "torch.Size([2, 2, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "# Mask도 변경\n",
        "print(attn_mask.size())\n",
        "attn_mask = attn_mask.unsqueeze(1).repeat(1, n_head, 1, 1)\n",
        "print(attn_mask.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubh168SyBxnw"
      },
      "source": [
        "##### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "ufWp4KXo6yyi",
        "outputId": "52021897-5830-42e7-d407-fd5b2dff8876"
      },
      "outputs": [],
      "source": [
        "scaled_dot_attn = ScaledDotProductAttention(d_head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "context, attn_prob =scaled_dot_attn.forward(q_s,k_s,v_s,attn_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 8, 64])\n",
            "torch.Size([2, 2, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "print(context.size())\n",
        "print(attn_prob.size() )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0zRmgcEfB1lM"
      },
      "source": [
        "##### Concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "EjQQPFsn7U8q",
        "outputId": "a3ac5624-652d-46bc-b5e3-b98f37b949e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "# (bs[배치사이즈], n_head[헤드 수], n_seq[최대길이], d_head[헤드별 디멘션] )\n",
        "# (bs[배치사이즈], n_seq[최대길이], n_head * d_head[헤드별 디멘션] )\n",
        "# context.transpose(1,2) # 시퀀스와 헤드의 자리를 바꿔줌\n",
        "context = context.transpose(1,2).contiguous().view(batch_size, -1, n_head*d_head)\n",
        "print(context.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6bxvZK6uB4WS"
      },
      "source": [
        "##### Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "kz3C9ABv7st1",
        "outputId": "d0ad361d-e594-409e-e3d6-29ecf84a110b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "linear = nn.Linear(n_head * d_head, d_hidn) \n",
        "# (bs, n_seq, n_hidn)\n",
        "output = linear(context)\n",
        "print(output.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wwcq-wgvB8Hq"
      },
      "source": [
        "##### Implementation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pVQZieaCB7xp"
      },
      "outputs": [],
      "source": [
        "\"\"\" multi head attention \"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_hidn, n_head, d_head):\n",
        "        super().__init__()\n",
        "        # self 인자\n",
        "        self.d_hidn = d_hidn\n",
        "        self.d_head = d_head\n",
        "        self.n_head = n_head\n",
        "\n",
        "        # linear, sacled_dot_attn, linear\n",
        "        self.W_Q = nn.Linear(d_hidn, n_head * d_head)  \n",
        "        self.W_K = nn.Linear(d_hidn, n_head * d_head)\n",
        "        self.W_V = nn.Linear(d_hidn, n_head * d_head) \n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(d_head)\n",
        "        self.linear = nn.Linear(n_head * d_head, d_hidn)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask): \n",
        "        batch_size = Q.size(0)\n",
        "        # q_s: (bs, n_head, n_q_seq, d_head)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_head, self.d_head).transpose(1, 2)\n",
        "        # k_s: (bs, n_head, n_k_seq, d_head)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_head, self.d_head).transpose(1, 2)\n",
        "        # v_s: (bs, n_head, n_v_seq, d_head)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_head, self.d_head).transpose(1, 2)\n",
        "\n",
        "        # mask\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_head, 1, 1)\n",
        "              \n",
        "        # scaled dot\n",
        "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        context, attn_prob =self.scaled_dot_attn.forward(q_s,k_s,v_s,attn_mask)\n",
        "        \n",
        "        # concat\n",
        "        # (bs, n_q_seq, h_head * d_head)\n",
        "        context = context.transpose(1,2).contiguous().view(batch_size, -1, self.n_head * self.d_head)\n",
        "        \n",
        "        # linear\n",
        "        # (bs, n_q_seq, d_hidn)\n",
        "        output = self.linear(context)\n",
        "        \n",
        "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        return output, attn_prob "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 128])\n",
            "torch.Size([2, 2, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "mul_head_attn = MultiHeadAttention(d_hidn, n_head, d_head)\n",
        "output, attn_prob = mul_head_attn.forward(Q,K,V,attn_mask)\n",
        "print(output.size())\n",
        "print(attn_prob.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4O7PH-v-SA9B"
      },
      "source": [
        "#### 8. Masked Multi Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "colab_type": "code",
        "id": "sg1uvsXJSAJE",
        "outputId": "d4bcd74c-7bb5-439b-bf71-2594cd4d850a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True]])\n",
            "tensor([[0, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "tensor([[[False,  True,  True,  True,  True,  True,  True,  True],\n",
            "         [False, False,  True,  True,  True,  True,  True,  True],\n",
            "         [False, False, False,  True,  True,  True,  True,  True],\n",
            "         [False, False, False, False,  True,  True,  True,  True],\n",
            "         [False, False, False, False, False,  True,  True,  True],\n",
            "         [False, False, False, False, False, False,  True,  True],\n",
            "         [False, False, False, False, False, False,  True,  True],\n",
            "         [False, False, False, False, False, False,  True,  True]],\n",
            "\n",
            "        [[False,  True,  True,  True,  True,  True,  True,  True],\n",
            "         [False, False,  True,  True,  True,  True,  True,  True],\n",
            "         [False, False, False,  True,  True,  True,  True,  True],\n",
            "         [False, False, False, False,  True,  True,  True,  True],\n",
            "         [False, False, False, False, False,  True,  True,  True],\n",
            "         [False, False, False, False, False, False,  True,  True],\n",
            "         [False, False, False, False, False, False, False,  True],\n",
            "         [False, False, False, False, False, False, False, False]]])\n"
          ]
        }
      ],
      "source": [
        "\"\"\" attention decoder mask \"\"\"\n",
        "def get_attn_decoder_mask(seq):\n",
        "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
        "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular\n",
        "    return subsequent_mask\n",
        "\n",
        "\n",
        "Q = input_sums\n",
        "K = input_sums\n",
        "V = input_sums\n",
        "\n",
        "# attn_pad_mask : 기존 input에 대한 pad mask\n",
        "attn_pad_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1)) # 차원수 늘리고, 기본적인 마스킹\n",
        "print(attn_pad_mask[0]) # 같은 문장에 대해 8개를 만듦\n",
        "\n",
        "\n",
        "# attn_dec_mask : 현재 단어에서 이전 단어만 보겠다는 attention mask\n",
        "\n",
        "attn_dec_mask = get_attn_decoder_mask(inputs)\n",
        "print(attn_dec_mask[0])\n",
        "# attn_mask : attn_pad_mask + attn_dec_mask\n",
        "attn_mask = torch.gt((attn_pad_mask + attn_dec_mask),0)\n",
        "print(attn_mask)\n",
        "\n",
        "\n",
        "batch_size = Q.size(0)\n",
        "n_head = 2\n",
        "\n",
        "\n",
        "# 마스킹 된값을 1 로 표현\n",
        "\n",
        "#  입력 시퀀스의 패딩 부분을 마스킹하고,\n",
        "# 뒤에 단어는 보지 않겠다(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "-YdCqAETUSzv",
        "outputId": "c927ad28-8967-4fc8-c614-6324467fa053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 128]) torch.Size([2, 2, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "attention = MultiHeadAttention(d_hidn, n_head, d_head) # 초기화\n",
        "output, attn_prob = attention(Q, K, V, attn_mask )\n",
        "print(output.size(), attn_prob.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xPiW76dzGy8D"
      },
      "source": [
        "#### 9. Feed Forward"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nLWqtn7UHGz-"
      },
      "source": [
        "##### f1 (Linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "U0iAPIfCHGaa",
        "outputId": "484ee583-3dc9-4401-9007-6f4c79c8cb7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
            "torch.Size([2, 512, 8])\n"
          ]
        }
      ],
      "source": [
        "# Linear : Conv1d로 활용\n",
        "conv1 = nn.Conv1d(in_channels=d_hidn, out_channels=d_hidn*4, kernel_size=1)\n",
        "print(conv1)\n",
        "ff_1 = conv1(output.transpose(1,2))\n",
        "print(ff_1.size())\n",
        "# (bs, d_hidn * 4, n_seq)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezEDjlVtIqf0"
      },
      "source": [
        "##### Activation (relu or gelu)\n",
        "\n",
        "![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/activation.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "x7mTDJZiIOrL"
      },
      "outputs": [],
      "source": [
        "# active = F.gelu\n",
        "active = F.gelu\n",
        "ff_2 = active(ff_1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fAcrKhAtIvn5"
      },
      "source": [
        "##### f3 (Linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "xGRsfIOcOgLR",
        "outputId": "427c31fb-2691-4524-f382-c0294143286c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "# Linear : Conv1d로 활용\n",
        "# 원형으로 다시 복원하는 과정\n",
        "conv2 = nn.Conv1d(in_channels=d_hidn*4, out_channels=d_hidn, kernel_size=1)\n",
        "ff3 = conv2(ff_2).transpose(1,2)\n",
        "print(ff3.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sE6-A3OhIzLo"
      },
      "source": [
        "##### Implementation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, d_hidn):\n",
        "        super().__init__()\n",
        "        self.d_hidn = d_hidn\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.d_hidn, out_channels=self.d_hidn*4, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.d_hidn*4, out_channels=self.d_hidn, kernel_size=1)\n",
        "        self.active = F.gelu\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # f1 output: (bs, d_ff, n_seq)\n",
        "        ff_1 = self.conv1(inputs.transpose(1,2))\n",
        "        # f2 output: (bs, n_seq, d_hidn)\n",
        "        active = F.gelu\n",
        "        ff_2 = self.active(ff_1)\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        ff3 = self.conv2(ff_2).transpose(1,2)\n",
        "        return ff3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, d_hidn):\n",
        "        super().__init__()\n",
        "        self.d_hidn = d_hidn\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.d_hidn, out_channels=self.d_hidn*4, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.d_hidn*4, out_channels=self.d_hidn, kernel_size=1)\n",
        "        self.active = F.gelu\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # f1 output: (bs, d_ff, n_seq)\n",
        "        output = self.active(self.conv1(inputs.transpose(1,2)))\n",
        "        # f2 output: (bs, n_seq, d_hidn)\n",
        "        active = F.gelu\n",
        "        output= self.conv2(output).transpose(1,2)\n",
        "        # (bs, n_seq, d_hidn)\n",
        "    \n",
        "        return output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transformer-01.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_lecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "305494ec69d5ad97a583cc76e8fd52e450123bc765c435a27726a289dbe2d5e0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
